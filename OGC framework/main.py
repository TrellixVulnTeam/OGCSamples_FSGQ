import argparse
import numpy as np
import json
import os #for checkpoint management

#tag
k1=0
k2=0
with open("./params/train_params.json", 'r') as f:
    train_params = json.load(f)
os.environ['CUDA_VISIBLE_DEVICES'] = train_params['gpu_number']
# device_ids=[0,1]


log=open("trainlog.txt")
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.utils import clip_grad_norm_

from data_iter import real_data_loader, dis_narrow_data_loader
from utils import recurrent_func, loss_func, get_sample, get_rewards,change_lr,D_gpu,G_gpu,C_gpu
from Discriminator import Discriminator
from Generatorinit import Generator
from test_folder.target import  Target,batch_nll_loss,generate_real_sample
from dis2 import Discriminator_Absolute
from interval2onehot import  interval_to_real
from critic import Critic

#Arguments


log = open("trainlog.txt", "r+")
log.truncate()
log = open("trainlog.txt", "a")
import time
def print1(input,log=log):
    str_intput=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))+" "+str(input)
    print(str_intput)
    # log.write(str+"\n")
    # log = open("trainlog.txt", "w")
    # log.write("strr")
    log.write(str_intput+'\n')
    log.flush()
    return



parser = argparse.ArgumentParser(description="LeakGAN")
parser.add_argument("--hpc", action="store_true", default=False)
parser.add_argument("--data_path", type=str, default="/save_files/", metavar="PATH", 
                    help="Data path to save files (default: /save_files/)")
parser.add_argument("--rounds", type=int, default=150, metavar="N",
                    help="Rounds of adversarial training (default:150)")
parser.add_argument("--g_pretrain_steps", type=int, default=120, metavar="N",
                    help="Steps of pre-training generator (defaul: 120)")                    
parser.add_argument("--d_pretrain_steps", type=int, default=50, metavar="N",
                    help="Steps of pre-training discriminator (defaul: 50)")    
parser.add_argument("--g_steps", type=int, default=1, metavar="N", 
                    help="Steps of generator updates in one round of adversarial training (defaul: 1)") #gen_train_num
parser.add_argument("--d_steps", type=int, default=3, metavar="N",
                    help="Steps of discriminator updates in one round of adversarial training (defaul: 3)")      
parser.add_argument("--gk_epochs", type=int, default=1, metavar="N",
                    help="Epochs of generator updates in one step of generate update (defaul: 1)")        
parser.add_argument("--dk_epochs", type=int, default=3, metavar="N",
                    help="Epochs of discriminator updates in one step of generate update (defaul: 3)")  
parser.add_argument("--update_rate", type=float, default=0.8, metavar="UR",
                    help="Update rate of rollout model (defaul: 0.8)")
parser.add_argument("--n_rollout", type=int, default=16, metavar="N",
                    help="Number of rollouts (defaul: 16)") #rollout_num
parser.add_argument("--vocab_size", type=int, default=10, metavar="N",
                    help="Vocabulary size (defaul: 10)")
parser.add_argument("--batch_size", type=int, default=64, metavar="N",
                    help="Batch size(defaul: 64)")
parser.add_argument("--n_samples", type=int, default=6400, metavar="N",
                    help="Number of samples generated per time(defaul: 6400)")
parser.add_argument("--gen_lr", type=float, default=1e-3, metavar="LR",
                    help="Learning Rate of generator optimizer (defaul: 1e-3)")
parser.add_argument("--dis_lr", type=float, default=1e-3, metavar="LR",
                    help="Learning Rate of discriminator optimizer (defaul: 1e-3)")
parser.add_argument("--no_cuda", action="store_true", default=False,
                    help="Disable CUDA training (defaul: False)")                                                                        
parser.add_argument("--seed", type=int, default=1, metavar="S",
                    help="Random seed (defaul: 1)")


f = open("./params/train_params.json")
train_params = json.load(f)
f.close()

def get_params(filePath):
    with open(filePath, 'r') as f:
        params = json.load(f)
    f.close()
    return params

def get_target_net():         #before get_target_net,make sure the train_data is generated by this target_net
                             #this function just for eval training
    with open("./params/target_params.json", 'r') as f:
        params = json.load(f)
    f.close()
    target_net= Target(**params)
    return target_net

def get_arguments():
    train_params = get_params("./params/train_params.json")
    leak_gan_params = get_params("./params/leak_gan_params.json")
    target_params = get_params("./params/target_params.json")
    dis_data_params = get_params("./params/dis_data_params.json")
    real_data_params = get_params("./params/real_data_params.json")
    return {
        "train_params": train_params,
        "leak_gan_params": leak_gan_params,
        "target_params": target_params,
        "dis_data_params": dis_data_params,
        "real_data_params" : real_data_params
    }
#List of models
def prepare_model_dict(use_cuda=True):
    f = open("./params/leak_gan_params.json")
    params = json.load(f)
    f.close()
    discriminator_params = params["discriminator_params"]
    generator_params = params["generator_params"]
    worker_params = generator_params["worker_params"]
    manager_params = generator_params["manager_params"]
    critic_params = params["critic_params"]
    discriminator_absolute_params = params['discriminator_absolute_params']




    worker_params["goal_out_size"] = discriminator_params["goal_out_size"]
    manager_params["goal_out_size"] = discriminator_params["goal_out_size"]


    discriminator = Discriminator(**discriminator_params)
    generator = Generator(worker_params, manager_params,
                          generator_params["step_size"])

    discriminator_absolute =Discriminator_Absolute(**discriminator_absolute_params)
    critic = Critic(**critic_params)


    with open("./params/target_params.json", 'r') as f:
        target_params = json.load(f)

    target=None
    if(target_params["use_target"]):
        target=get_target_net()#tag  fixed net

    is_load_pre_dis,pre_dis_path=train_params['is_load_pre_dis'],train_params['pre_dis_path']

    is_load_pre_dis2,pre_dis2_path=train_params['is_load_pre_dis2'],train_params['pre_dis2_path']

    is_load_pre_gen,pre_gen_path=train_params['is_load_pre_gen'],train_params['pre_gen_path']

    is_load_pre_critic, pre_critic_path = train_params['is_load_pre_critic'], train_params['pre_critic_path']

    is_load_adv_dis,adv_load_dis_path =train_params['is_load_adv_dis'],train_params['adv_load_dis_path']
    is_load_adv_gen,adv_load_gen_path =train_params['is_load_adv_gen'],train_params['adv_load_gen_path']
    is_load_adv_critic,adv_load_critic_path =train_params['is_load_adv_critic'],train_params['adv_load_critic_path']



    if(is_load_pre_dis):
        if(is_load_adv_dis):
            discriminator.load_state_dict(torch.load(adv_load_dis_path))
            print1("---------------load adv dis-------------"+str(adv_load_dis_path))
        else:
            discriminator.load_state_dict(torch.load(pre_dis_path))
            print1("---------------load pre dis-------------")

    if (is_load_pre_dis2):

        discriminator_absolute.load_state_dict(torch.load(pre_dis2_path))
        print1("---------------load pre dis2-------------")


    if (is_load_pre_gen):
        if (is_load_adv_gen):
            generator.load_state_dict(torch.load(adv_load_gen_path))
            print1("---------------load adv gen-------------" + str(adv_load_gen_path))
        else:
            generator.load_state_dict(torch.load(pre_gen_path))
            # generator.worker._init_params() #tag
            print1("---------------load pre gen-------------")

    if (is_load_pre_critic):
        if (is_load_adv_critic):
            critic.load_state_dict(torch.load(adv_load_critic_path))
            print1("---------------load adv critic-------------" + str(adv_load_critic_path))
        else:
            critic.load_state_dict(torch.load(pre_critic_path))
            # generator.worker._init_params() #tag
            print1("---------------load pre critic-------------")

    # "is_load_adv_dis": 1,
    # "is_load_adv_gen":1,
    # "is_load_adv_critic": 1,
    # 
    # "adv_load_gen_path": "./pts/adv_gen9.pt",
    # "adv_load_dis_path": "./pts/adv_dis9.pt",
    # "adv_load_critic_path":"./pts/adv_critic9.pt",











    if use_cuda:
        # generator.worker = generator.worker.cuda()
        # generator.manager = generator.manager.cuda()
        generator=generator.cuda(G_gpu)
        discriminator_absolute_params=discriminator_absolute.cuda(D_gpu)
        discriminator = discriminator.cuda(D_gpu)
        critic = critic.cuda(C_gpu)



        if (target_params["use_target"]):
            target=target.cuda()
    model_dict = {"generator": generator, "discriminator": discriminator,"critic":critic,"discriminator_absolute":discriminator_absolute,"target":target}#tag

    print("Total number of obverser paramerters in networks is {}  ".format(
        sum(x.numel() for x in model_dict['discriminator'].parameters())))
    print("Total number of critic paramerters in networks is {}  ".format(
        sum(x.numel() for x in model_dict['critic'].parameters())))
    return model_dict

#List of optimizers
def prepare_optimizer_dict(model_dict, lr_dict): #lr_dict = learning rate 
    generator = model_dict["generator"]
    discriminator = model_dict["discriminator"]
    discriminator_absolute = model_dict["discriminator_absolute"]
    critic=model_dict['critic']

    worker = generator.worker
    manager = generator.manager

    m_lr = lr_dict["manager"]
    w_lr = lr_dict["worker"]
    d_lr = lr_dict["discriminator"]
    d2_lr = lr_dict["discriminator_absolute"]

    #tag  ,æå®
    c_lr = lr_dict["discriminator"]



    w_optimizer = optim.Adam(worker.parameters(), lr=w_lr)
    m_optimizer = optim.Adam(manager.parameters(), lr=m_lr)
    d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr)
    d2_optimizer = optim.Adam(discriminator_absolute.parameters(),lr=d2_lr)
    c_optimizer  = optim.Adam(critic.parameters(),lr=c_lr)

    return {"worker": w_optimizer, "manager": m_optimizer,
            "discriminator": d_optimizer,"discriminator_absolute":d2_optimizer,"critic":c_optimizer}

#List of Learning rate Schedulers
def prepare_scheduler_dict(optmizer_dict, step_size=200, gamma=0.99):
    w_optimizer = optmizer_dict["worker"]
    m_optimizer = optmizer_dict["manager"]
    d_optimizer = optmizer_dict["discriminator"]
    d2_optimizer = optmizer_dict['discriminator_absolute']
    c_optimizer = optmizer_dict['critic']



    w_scheduler = optim.lr_scheduler.StepLR(w_optimizer, step_size=step_size,
                                            gamma=gamma)
    m_scheduler = optim.lr_scheduler.StepLR(m_optimizer, step_size=step_size,
                                            gamma=gamma)

    dgamma=0.99
    d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=step_size,
                                            gamma=dgamma)
    c_scheduler = optim.lr_scheduler.StepLR(c_optimizer, step_size=step_size,
                                            gamma=gamma)

    d2_scheduler = optim.lr_scheduler.StepLR(d2_optimizer,step_size=step_size,gamma=gamma)
    return {"worker": w_scheduler, "manager": m_scheduler,
            "discriminator": d_scheduler,"discriminator_absolute":d2_scheduler,"critic":c_scheduler}

#Pretraining the Generator
def pretrain_generator(curepoch,is_load_pre_gen,pre_gen_path,model_dict, optimizer_dict, scheduler_dict, dataloader, vocab_size,eval_data_params,eval_file,eval_batchs_number,max_norm=5.0, use_cuda=True, epoch=1, tot_epochs=100,temperature=1.0):

  #get the models of generator
    generator = model_dict["generator"]
    worker = generator.worker
    manager = generator.manager
    #get the optimizers
    m_optimizer = optimizer_dict["manager"]
    w_optimizer = optimizer_dict["worker"]
    
    m_optimizer.zero_grad()
    w_optimizer.zero_grad()

    m_lr_scheduler = scheduler_dict["manager"]
    w_lr_scheduler = scheduler_dict["worker"]
    init_eval_file = eval_file
    name, suffix = os.path.splitext(init_eval_file)
    """
     Perform pretrain step for real data
    """

    print_m_loss=0


    print_w_loss = 0
    for i, sample in enumerate(dataloader):
        #print("DataLoader: {}".format(dataloader))
        m_lr_scheduler.step()
        w_lr_scheduler.step()

        sample = Variable(sample)
        if use_cuda:
            sample = sample.cuda(async=True)
        if (sample.size() == torch.zeros([64, 128]).size()): #sometimes smaller than 64 (16) is passed, so this if statement disables it
            #print("Sample size: {}".format(sample.size()))
            pre_rets = recurrent_func("pre")(model_dict, sample, use_cuda)
            # pre_rets = recc_pre(model_dict, sample, use_cuda)
            real_goal = pre_rets["real_goal"]
            prediction = pre_rets["prediction"]
            delta_feature = pre_rets["delta_feature"]

        #tag test
            # torch.zero_(real_goal)  # m_loss ===-1  delta_feature similar to all zero. only some has value
            # torch.zero_(delta_feature)
            # real_goal=torch.ones_like(real_goal)

            #is_m_w_difference(manager,worker)    no overlap para
            m_loss = loss_func("pre_manager")(real_goal, delta_feature)
            # set_required_grad(worker.parameters(), False)
            torch.autograd.backward(m_loss, manager.parameters(), retain_graph=True)
            # m_loss.backward()
            clip_grad_norm_(manager.parameters(), max_norm=max_norm)
            # set_required_grad(worker.parameters(), True)
            m_optimizer.step()
            m_optimizer.zero_grad()



            w_loss = loss_func("pre_worker")(sample, prediction, vocab_size, use_cuda)
            # set_required_grad(manager.parameters(), False)

            torch.autograd.backward(w_loss, worker.parameters(),retain_graph=False)
            # torch.autograd.backward(m_loss, manager.parameters(), retain_graph=False)
            # set_required_grad(manager.parameters(), True)
            clip_grad_norm_(worker.parameters(), max_norm=max_norm)
            #tag
            # m_optimizer.zero_grad()
            m_optimizer.zero_grad()
            w_optimizer.step()
            w_optimizer.zero_grad()


            del real_goal
            del prediction
            del delta_feature
            del pre_rets

            # generator.worker = worker
            # generator.manager = manager
            # model_dict["generator"] = generator
            print_m_loss+=m_loss.item()
            print_w_loss += w_loss.item()
            del m_loss
            del w_loss
            print_epoch=1


            save_subepoch =50
            if i % save_subepoch == 0:
                curname = str(curepoch) + "_" + str(i)
                torch.save(generator.state_dict(), os.path.dirname(pre_gen_path)+ "/ptdetail/"+ curname)

            if i%print_epoch==0:
                if i==0:
                    print_m_loss*=print_epoch
                    print_w_loss *= print_epoch
                print1("{:5f}:{:5f}:Pre-Manager Loss: {:.5f}, Pre-Worker Loss: {:.5f}\\n".format(curepoch, i, print_m_loss/print_epoch,print_w_loss / print_epoch))
                # print1("Pre-Manager Loss: {:.5f}, Pre-Worker Loss: {:.5f}\\n".format(print_m_loss/print_epoch, print_w_loss/print_epoch))
                print_m_loss=0
                print_w_loss=0
            # if i%30==0:
            #     torch.save(generator.state_dict(), pre_gen_path)
            # if m_loss < 0.78 and w_loss < 0.025:
            #     torch.save(generator.state_dict(), pre_gen_path + "np" )
            #
            # elif m_loss < 0.8and w_loss < 0.03 :
            #     torch.save(generator.state_dict(), pre_gen_path + "bb")
            if i==99999999:
                # eval_file=str(i)+str(i//30)+eval_file

                curname=name+str(curepoch)+"_"+str(i//300)


                eval_file=curname+suffix
                generate_samples(model_dict, eval_file, eval_batchs_number, use_cuda, temperature)
                   # model_dict['target']
                #
                # real_data_loader(filepath, batch_size, shuffle, num_workers, pin_memory)
                # dis_dataloader_params["positive_filepath"] = positive_file
                # dis_dataloader_params["negative_filepath"] = negative_file
                # # print(dis_dataloader_params)


                with open("./params/target_params.json", 'r') as f:
                    target_params = json.load(f)
                if (target_params["use_target"]):
                    dataloader = real_data_loader(**eval_data_params)
                    nll_loss=batch_nll_loss(model_dict['target'],dataloader,use_cuda=use_cuda)
                else:
                    nll_loss=0

                print1("Pre-Manager Loss: {:.5f}, Pre-Worker Loss: {:.5f},NLL-loss:{:.5f}\n".format(m_loss, w_loss,nll_loss))
                # torch.save(generator.state_dict(), pre_gen_path)
       #model_dict used to get w_loss m_loss

        # break#tag ,test save



        """
    Update model_dict, optimizer_dict, and scheduler_dict
    """





    optimizer_dict["manager"] = m_optimizer
    optimizer_dict["worker"] = w_optimizer

    scheduler_dict["manager"] = m_lr_scheduler
    scheduler_dict["worker"] = w_lr_scheduler


    torch.save(generator.state_dict(), pre_gen_path)


    return model_dict, optimizer_dict, scheduler_dict

def generate_samples(model_dict, negative_file, batch_size,
                     use_cuda=False, temperature=1.0):
    with torch.no_grad():
        neg_data = []
        for _ in range(batch_size): # batch_size>100 too slow,why ?
            sample = get_sample(model_dict, use_cuda, temperature)
            sample = sample.cpu()
            neg_data.append(sample.data.numpy())
        if not(batch_size==0):
            neg_data = np.concatenate(neg_data, axis=0)
            np.save(negative_file, neg_data)




def pretrain_dis2(model_dict,optimizer_dict,scheduler_dict,postive_absolute_file,neg_absolute_file,dis_dataloader_params,pre_dis2_path,epochs=100,use_cuda=True):
    disciminator_absolute=model_dict["discriminator_absolute"]
    d2_optimizer=optimizer_dict["discriminator_absolute"]
    d2_lr_scheduler=scheduler_dict['discriminator_absolute']

    name, suffix = os.path.splitext(neg_absolute_file)
    namepre=name+"pre"
    neg_file_pre=name+suffix

    #1
    # generate_samples(model_dict,neg_file_pre,156, True, 1)
    #get absolute

    # neg=np.load(neg_file_pre)
    # neg_real=interval_to_real(neg)
    # np.save(neg_absolute_file,neg_real)
    # del neg
    # del neg_real
    dis_dataloader_params["positive_filepath"] = postive_absolute_file
    dis_dataloader_params["negative_filepath"] = neg_absolute_file


    cross_entropy = nn.CrossEntropyLoss()  # this one is similar to NLL (negative log likelihood)
    if use_cuda:
        cross_entropy = cross_entropy.cuda()


    for epoch in range(epochs):
        dataloader = dis_narrow_data_loader(**dis_dataloader_params)
        print_loss = 0
        for i, sample in enumerate(dataloader):
            data, label = sample["data"], sample["label"]
            if(data.shape[0]==64):
                d2_optimizer.zero_grad()
                 #initialize sample variables
                data = Variable(data)
                label = Variable(label)
                if use_cuda:
                    data = data.cuda()
                    label = label.cuda()
                outs = disciminator_absolute(data)
                loss = cross_entropy(outs, label.view(-1))
                d2_lr_scheduler.step()
                loss.backward()
                d2_optimizer.step()
                print_loss+=loss.item()
                if i %63==0 :
                    if i==0:
                        print_loss*=63

                    print1("epoch:{:.5f}-{:.5f}Pre-Dis2 loss: {:.5f},".format(epoch,i,print_loss/63))
                    print_loss=0
        del dataloader






    torch.save(disciminator_absolute.state_dict(), pre_dis2_path)

    model_dict["discriminator_absolute"] = disciminator_absolute
    optimizer_dict["discriminator_absolute"] = d2_optimizer
    scheduler_dict["discriminator_absolute"] = d2_lr_scheduler
    return model_dict, optimizer_dict, scheduler_dict







def pretrain_discriminator(is_load_pre_dis,pre_dis_path,model_dict, optimizer_dict, scheduler_dict,
                           dis_dataloader_params, vocab_size, positive_file,
                           negative_file, batch_size, epochs, use_cuda=True, temperature=1.0):
    discriminator = model_dict["discriminator"]
    d_optimizer = optimizer_dict["discriminator"]
    d_lr_scheduler = scheduler_dict["discriminator"]



    #
    # # batch_size=10
    # batch_size=3
    generate_samples(model_dict, negative_file, batch_size, use_cuda, temperature)
    dis_dataloader_params["positive_filepath"] = positive_file
    dis_dataloader_params["negative_filepath"] = negative_file
    #print(dis_dataloader_params)


    cross_entropy = nn.CrossEntropyLoss() #this one is similar to NLL (negative log likelihood)
    if use_cuda:
        cross_entropy = cross_entropy.cuda()


    for epoch in range(epochs):
        dataloader = dis_narrow_data_loader(**dis_dataloader_params)  # this is where data iterator is used
        print_loss=0
        for i, sample in enumerate(dataloader):
            d_optimizer.zero_grad()
            data, label = sample["data"], sample["label"] #initialize sample variables
            data = Variable(data)
            label = Variable(label)
            if use_cuda:
                data = data.cuda(D_gpu)
                label = label.cuda(D_gpu)
            outs = discriminator(data)
            loss = cross_entropy(outs["score"], label.view(-1)) + discriminator.l2_loss()

            #don't change d_lr
            d_lr_scheduler.step()


            loss.backward()
            d_optimizer.step()
            print_loss=print_loss+loss.item()
            if i % 63== 0:
                # if i%5==0:
                if i == 0:
                    print_loss *= 63
                print1("Pre-Discriminator loss: {:.5f}".format(print_loss/63))
                print_loss=0






    torch.save(discriminator.state_dict(), pre_dis_path)

    model_dict["discriminator"] = discriminator
    optimizer_dict["discriminator"] = d_optimizer
    scheduler_dict["discriminator"] = d_lr_scheduler
    return model_dict, optimizer_dict, scheduler_dict

#Adversarial training 
def adversarial_train(cur_epoch,eval_file, eval_batchs_number,eval_data_params,real_eval_data_params,model_dict, optimizer_dict, scheduler_dict, dis_dataloader_params,dis2_dataloader_params,
                      vocab_size, pos_file, neg_file, batch_size, gen_train_num=3,
                      dis_train_epoch=1, dis_train_num=50, max_norm=5.0,
                      rollout_num=4, use_cuda=False, temperature=1.0, epoch=1, tot_epoch=100):
    """
        Get all the models, optimizer and schedulers
    """
    print("------a--------")
    init_eval_file = eval_file
    name, suffix = os.path.splitext(init_eval_file)



    generator = model_dict["generator"]
    discriminator = model_dict ["discriminator"]
    critic = model_dict["critic"]
    worker = generator.worker
    manager = generator.manager


    m_optimizer = optimizer_dict["manager"]
    w_optimizer = optimizer_dict["worker"]
    c_optimizer = optimizer_dict["critic"]
    d_optimizer = optimizer_dict["discriminator"]



    #Why zero grad only m and w?
    m_optimizer.zero_grad()
    w_optimizer.zero_grad()

    m_lr_scheduler = scheduler_dict["manager"]
    w_lr_scheduler = scheduler_dict["worker"]
    c_lr_scheduler = scheduler_dict["critic"]

    #Adversarial training for generator
    # gen_train_num=0#tag
    # for _ in range(gen_train_num):
    # for _ in range(2):
    for _ in range(2):
        #tag  break

        # break
        # for p in manager.parameters():  # backward w_loss will change grap value of manageer.para()
        #     p.requires_grad =

        #don't change rl
        # m_lr_scheduler.step()
        # w_lr_scheduler.step()

        m_optimizer.zero_grad()
        w_optimizer.zero_grad()

        #get all the return values
        adv_rets = recurrent_func("adv")(model_dict, use_cuda)
        real_goal = adv_rets["real_goal"]
        all_goal = adv_rets["all_goal"]
        prediction = adv_rets["prediction"]
        delta_feature = adv_rets["delta_feature"]
        delta_feature_for_worker = adv_rets["delta_feature_for_worker"]
        gen_token = adv_rets["gen_token"]



        rewards = get_rewards(model_dict, gen_token, rollout_num, use_cuda)




        m_loss,distance = loss_func("adv_manager")(rewards, real_goal, delta_feature)
        # m_loss,distance = loss_func("adv_manager")(real_goal, delta_feature)
        w_loss = loss_func("adv_worker")(rewards,real_goal,delta_feature,all_goal, delta_feature_for_worker, gen_token, prediction, vocab_size, use_cuda)

        # w_loss=0



        # set_required_grad(worker.parameters(), False)
        torch.autograd.backward(m_loss, generator.parameters(),retain_graph=True) #based on loss improve the parameters
        # set_required_grad(worker.parameters(), True)
        #why use manager.parameter(),  grad =0 after loading pretrain model,tag to check
        clip_grad_norm_(manager.parameters(), max_norm)
        m_optimizer.step()
        m_optimizer.zero_grad()

        # set_required_grad(manager.parameters(), False)
        torch.autograd.backward(w_loss,generator.parameters(),retain_graph=False) #why  w has grad after this ,lr don't influence grad
        # set_required_grad(manager.parameters(), True)

        clip_grad_norm_(worker.parameters(), max_norm)
        #tttttttttttttttttttag
        w_optimizer.step()
        w_optimizer.zero_grad()










        with open("./params/target_params.json", 'r') as f:
            target_params = json.load(f)
        if (target_params["use_target"]):
            dataloader = real_data_loader(**eval_data_params)
            nll_loss = batch_nll_loss(model_dict['target'], dataloader, use_cuda=use_cuda)

            generate_real_sample(model_dict['target'], eval_batchs_number, real_eval_data_params['filepath'],
                                 use_cuda=use_cuda)
            dataloader_ground_truth = real_data_loader(**real_eval_data_params)
            ground_truth = batch_nll_loss(model_dict['target'], dataloader_ground_truth, use_cuda=use_cuda)

        else:
            nll_loss = 0
            ground_truth = 0
        # nll_loss = batch_nll_loss(model_dict['target'], dataloader, use_cuda=use_cuda)



        # def generate_real_sample(net, batchs, filepath, use_cuda=False):
        print1("Adv-Manager loss: {:.5f} Distantce:{:.5f} Adv-Worker loss: {:.5f} NLL-loss:{:.5f} Ground-Truth:{:.5f}".format(m_loss,distance, w_loss,nll_loss,ground_truth))
        #use model_dict to get feature,update per epoch
        generator.worker = worker
        generator.manager = manager
        model_dict["generator"] = generator



        del adv_rets
        del real_goal
        del all_goal
        del prediction
        del delta_feature
        del delta_feature_for_worker
        del gen_token
        del rewards
        del w_loss
        del m_loss
        curname = name + "adv_" + str(cur_epoch)+"_"+str(_)
        eval_file = curname + suffix

        generate_samples(model_dict, eval_file, 3, use_cuda, temperature)
        # generate_samples(model_dict, eval_file, eval_batchs_number, use_cuda, temperature)
    #after update generator

    print("------b---f-----")
    #Adversarial training for discriminator
    for n in range(1):
        # batch_size=10
        generate_samples(model_dict, neg_file, 156, use_cuda, temperature)
        # generate_samples(model_dict, neg_file, 0, use_cuda, temperature)


        #-------------sdfsdf
        neg = np.load(neg_file)
        neg_real = interval_to_real(neg)

        if (neg_real.shape[0] >= 64):  #else use the last sample.
            print("change file")
            np.save(dis2_dataloader_params['negative_filepath'], neg_real)
        else:print("the absolute sample is collapse,use the last one")



        dataloader = dis_narrow_data_loader(**dis2_dataloader_params)
        cross_entropy = nn.CrossEntropyLoss()  # this one is similar to NLL (negative log likelihood)
        if use_cuda:
            cross_entropy = cross_entropy.cuda()
        disciminator_absolute = model_dict["discriminator_absolute"]
        d2_optimizer = optimizer_dict["discriminator_absolute"]
        d2_lr_scheduler = scheduler_dict['discriminator_absolute']


        c_lr_scheduler =scheduler_dict['critic']

        for epoch in range(0): #tag
            for i, sample in enumerate(dataloader):
                data, label = sample["data"], sample["label"]
                if (data.shape[0] == 64):
                    d2_optimizer.zero_grad()
                    # initialize sample variables
                    data = Variable(data)
                    label = Variable(label)
                    if use_cuda:
                        data = data.cuda()
                        label = label.cuda()
                    outs = disciminator_absolute(data)

                    loss2 = cross_entropy(outs, label.view(-1))
                    # don't change rl
                    # d2_lr_scheduler.step()
                    loss2.backward()

                    d2_optimizer.step()
                    if i % 63 == 0:
                        print1(str(epoch)+":ADV-Discriminator_absolute loss: {:.5f}".format(loss2))
        print("line713")
        # adv_dis2_path="./pts/adv_dis2.pt"
        # torch.save(disciminator_absolute.state_dict(), adv_dis2_path)
        print("line716")
        model_dict["discriminator_absolute"] = disciminator_absolute
        optimizer_dict["discriminator_absolute"] = d2_optimizer
        scheduler_dict["discriminator_absolute"] = d2_lr_scheduler
        #-------------âtrain dis_absolute
        print("------c--------")
        del dataloader


        cross_entropy = nn.CrossEntropyLoss()
        if use_cuda:
            cross_entropy = cross_entropy.cuda()
        """
        for d-steps do
            Use current G, Î¸m,Î¸w to generate negative examples and combine with given positive examples S 
            Train discriminator DÏ for k epochs by Eq. (2)
        end for
        """

        dataloader = dis_narrow_data_loader(**dis_dataloader_params)
        for _ in range(dis_train_num):
            print_loss = 0
            for i, sample in enumerate(dataloader):
                data, label = sample["data"], sample["label"]
                if use_cuda:
                    data = data.cuda(C_gpu)
                    label=label.cuda(C_gpu)
                outs = critic(data)

                loss1 = cross_entropy(outs["score"], label.view(-1))
                # loss1 = cross_entropy(outs["score"], label.view(-1)) + critic.l2_loss()
                print_loss+=loss1.item()
                c_optimizer.zero_grad() #Generator will set Dis's grad,so first zero_grad
                # don't change rl
                # c_lr_scheduler.step()
                loss1.backward()


                c_optimizer.step()




                if (i % 63 == 0):
                    print1(str(print_loss / 63))
                    print_loss = 0
        del dataloader
            # print1("adv-critic-"+str(_)+" :"+str(allloss))
    #Save all changes
        # model_dict["discriminator"] = discriminator


        # optimizer_dict["manager"] = m_optimizer
        # optimizer_dict["worker"] = w_optimizer
        # optimizer_dict["discriminator"] = d_optimizer
        #
        #
        # scheduler_dict["manager"] = m_lr_scheduler
        # scheduler_dict["worker"] = w_lr_scheduler
        # scheduler_dict["discriminator"] = d_lr_scheduler
        print("------c--------")
        # del dataloader
    return model_dict, optimizer_dict, scheduler_dict


def main():
    """
    Get all parameters
    """
    param_dict = get_arguments()
    use_cuda = torch.cuda.is_available()
    #Random seed
    torch.manual_seed(param_dict["train_params"]["seed"])
    np.random.seed(param_dict["train_params"]["seed"])

    #Pretrain step
    checkpoint_path = param_dict["train_params"]["checkpoint_path"]

    model_dict = prepare_model_dict(use_cuda)
    lr_dict = param_dict["train_params"]["lr_dict"]
    optimizer_dict = prepare_optimizer_dict(model_dict, lr_dict)
    gamma = param_dict["train_params"]["decay_rate"]
    step_size = param_dict["train_params"]["decay_step_size"]
    scheduler_dict = prepare_scheduler_dict(optimizer_dict, gamma=gamma, step_size=step_size)


    for kkk in range(2000):
        print1 ("#########################################################################")
        print1 ("Start Pretraining Discriminator...")
        with open("./params/dis_data_params.json", 'r') as f:
            dis_data_params = json.load(f)
        if use_cuda:
            dis_data_params["pin_memory"] = True
        f.close()
        pos_file = dis_data_params["positive_filepath"]
        neg_file = dis_data_params["negative_filepath"]
        batch_size = param_dict["train_params"]["generated_batchs"]   #batch_size  is generated batch numbers
        vocab_size = param_dict["leak_gan_params"]["discriminator_params"]["vocab_size"]
        pre_dis_path=param_dict["train_params"]["pre_dis_path"]
        is_load_pre_dis=param_dict["train_params"]["is_load_pre_dis"]
        # if not train_params['is_load_pre_dis']:

        if kkk!=0:
            param_dict["train_params"]["pre_dis_epoch_num"] =2
            param_dict["train_params"]["pre_gen_epoch_num"] = 5
        else:
            print("pass first")

        for i in range(param_dict["train_params"]["pre_dis_epoch_num"]):
            print1("Epoch: {}/{}  Pre-Discriminator".format(i, param_dict["train_params"]["pre_dis_epoch_num"]))
            model_dict, optimizer_dict, scheduler_dict = pretrain_discriminator(is_load_pre_dis=is_load_pre_dis,pre_dis_path=pre_dis_path, model_dict=model_dict,optimizer_dict=optimizer_dict, scheduler_dict=scheduler_dict, dis_dataloader_params=dis_data_params, vocab_size=vocab_size, positive_file=pos_file, negative_file=neg_file, batch_size=batch_size, epochs=60, use_cuda=use_cuda)
            torch.save(model_dict['discriminator'].state_dict(), pre_dis_path + str(i))

        print1("Start Pretrainning Discriminator_absolute")

        with open("./params/dis_absolute_data_params.json", 'r') as f:
            dis2_data_params = json.load(f)
        if use_cuda:
            dis2_data_params["pin_memory"] = True
        f.close()
        pos_file2 = dis2_data_params["positive_filepath"]
        neg_file2 = dis2_data_params["negative_filepath"]
        pre_dis2_path = param_dict["train_params"]['pre_dis2_path']


        for i in range(param_dict["train_params"]["pre_dis2_epoch_num"]):
            model_dict, optimizer_dict, scheduler_dict=pretrain_dis2(model_dict,optimizer_dict,scheduler_dict,pos_file2,neg_file2,dis2_data_params,pre_dis2_path)
            torch.save(model_dict['discriminator_absolute'].state_dict(), pre_dis2_path + str(i))







        #Pretrain generator
        print1 ("#########################################################################")
        print1 ("Start Pretraining Generator...")
        real_data_params = param_dict["real_data_params"]
        if use_cuda:
            real_data_params["pin_memory"] = True
        r_dataloader = real_data_loader(**real_data_params)

        eval_file = param_dict["train_params"]["eval_filepath"]
        eval_batchs_number = param_dict["train_params"]["eval_generated_num"]
        is_load_pre_gen=param_dict["train_params"]["is_load_pre_gen"]
        pre_gen_path=param_dict["train_params"]["pre_gen_path"]
        with open("./params/eval_data_params.json", 'r') as f:
            eval_data_params = json.load(f)

        with open("./params/real_eval_data_params.json", 'r') as f:
            real_eval_data_params = json.load(f)

        # print(eval_data_params)

        f.close()
        for epoch in range(param_dict["train_params"]["pre_gen_epoch_num"]):
        # for epoch in range(1):
            print1("Epoch: {}/{}  Pre-Generator".format(epoch, param_dict["train_params"]["pre_gen_epoch_num"]))
            model_dict, optimizer_dict, scheduler_dict = pretrain_generator(epoch,is_load_pre_gen,pre_gen_path, model_dict,optimizer_dict, scheduler_dict, r_dataloader, vocab_size=vocab_size, eval_data_params=eval_data_params,eval_file= eval_file, eval_batchs_number=eval_batchs_number,use_cuda=use_cuda, epoch=epoch, tot_epochs=range(param_dict["train_params"]["pre_gen_epoch_num"]))

            generator=model_dict['generator']
            torch.save(generator.state_dict(), pre_gen_path+str(epoch))

    #Pretrain discriminator
    # print1 ("#########################################################################")
    # print1 ("Start Pretraining Discriminator...")
    # with open("./params/dis_data_params.json", 'r') as f:
    #     dis_data_params = json.load(f)
    # if use_cuda:
    #     dis_data_params["pin_memory"] = True
    # f.close()
    # pos_file = dis_data_params["positive_filepath"]
    # neg_file = dis_data_params["negative_filepath"]
    # batch_size = param_dict["train_params"]["generated_batchs"]   #batch_size  is generated batch numbers
    # vocab_size = param_dict["leak_gan_params"]["discriminator_params"]["vocab_size"]
    # pre_dis_path=param_dict["train_params"]["pre_dis_path"]
    # is_load_pre_dis=param_dict["train_params"]["is_load_pre_dis"]




    # if not train_params['is_load_pre_dis']:
    # for i in range(param_dict["train_params"]["pre_dis_epoch_num"]):
    #     print1("Epoch: {}/{}  Pre-Discriminator".format(i, param_dict["train_params"]["pre_dis_epoch_num"]))
    #     model_dict, optimizer_dict, scheduler_dict = pretrain_discriminator(is_load_pre_dis=is_load_pre_dis,pre_dis_path=pre_dis_path, model_dict=model_dict,optimizer_dict=optimizer_dict, scheduler_dict=scheduler_dict, dis_dataloader_params=dis_data_params, vocab_size=vocab_size, positive_file=pos_file, negative_file=neg_file, batch_size=batch_size, epochs=60, use_cuda=use_cuda)
    #     torch.save(model_dict['discriminator'].state_dict(), pre_dis_path + str(i))
    #
    #
    # print1("Start Pretrainning Discriminator_absolute")
    #
    # with open("./params/dis_absolute_data_params.json", 'r') as f:
    #     dis2_data_params = json.load(f)
    # if use_cuda:
    #     dis2_data_params["pin_memory"] = True
    # f.close()
    # pos_file2 = dis2_data_params["positive_filepath"]
    # neg_file2 = dis2_data_params["negative_filepath"]
    # pre_dis2_path = param_dict["train_params"]['pre_dis2_path']
    #
    #
    # for i in range(param_dict["train_params"]["pre_dis2_epoch_num"]):
    #     model_dict, optimizer_dict, scheduler_dict=pretrain_dis2(model_dict,optimizer_dict,scheduler_dict,pos_file2,neg_file2,dis2_data_params,pre_dis2_path)
    #     torch.save(model_dict['discriminator_absolute'].state_dict(), pre_dis2_path + str(i))
    #
    #
    #
    #
    #
    #
    #
    # #Pretrain generator
    # print1 ("#########################################################################")
    # print1 ("Start Pretraining Generator...")
    # real_data_params = param_dict["real_data_params"]
    # if use_cuda:
    #     real_data_params["pin_memory"] = True
    # r_dataloader = real_data_loader(**real_data_params)
    #
    # eval_file = param_dict["train_params"]["eval_filepath"]
    # eval_batchs_number = param_dict["train_params"]["eval_generated_num"]
    # is_load_pre_gen=param_dict["train_params"]["is_load_pre_gen"]
    # pre_gen_path=param_dict["train_params"]["pre_gen_path"]
    # with open("./params/eval_data_params.json", 'r') as f:
    #     eval_data_params = json.load(f)
    #
    # with open("./params/real_eval_data_params.json", 'r') as f:
    #     real_eval_data_params = json.load(f)
    #
    # # print(eval_data_params)
    #
    # f.close()
    # for epoch in range(param_dict["train_params"]["pre_gen_epoch_num"]):
    # # for epoch in range(1):
    #     print1("Epoch: {}/{}  Pre-Generator".format(epoch, param_dict["train_params"]["pre_gen_epoch_num"]))
    #     model_dict, optimizer_dict, scheduler_dict = pretrain_generator(epoch,is_load_pre_gen,pre_gen_path, model_dict,optimizer_dict, scheduler_dict, r_dataloader, vocab_size=vocab_size, eval_data_params=eval_data_params,eval_file= eval_file, eval_batchs_number=eval_batchs_number,use_cuda=use_cuda, epoch=epoch, tot_epochs=range(param_dict["train_params"]["pre_gen_epoch_num"]))
    #
    #     generator=model_dict['generator']
    #     torch.save(generator.state_dict(), pre_gen_path+str(epoch))



    print1 ("#########################################################################")
    print1("Start Adversarial Training...")
    vocab_size = param_dict["leak_gan_params"]["discriminator_params"]["vocab_size"]


    with open("./params/train_params.json", 'r') as f:
        train_params = json.load(f)

    w=optimizer_dict['worker']
    m=optimizer_dict['manager']
    d=optimizer_dict['discriminator']

    rl_lr_dict= train_params['rl_lr_dict']
    m_lr = rl_lr_dict["manager"]
    w_lr = rl_lr_dict["worker"]
    d_lr = rl_lr_dict["discriminator"]

    change_lr(m,m_lr)
    change_lr(w,w_lr)
    change_lr(d,d_lr)

    with open("./params/train_params.json", 'r') as f:
        train_params = json.load(f)
    adv_gen_name=train_params["adv_gen_path"]
    adv_dis_name=train_params["adv_dis_path"]
    adv_dis2_name=train_params["adv_dis2_path"]
    adv_critic_name= train_params['adv_critic_path']


    gen_name, gen_suffix = os.path.splitext(adv_gen_name)
    dis_name, dis_suffix = os.path.splitext(adv_dis_name)
    disa_name,disa_suffix = os.path.splitext(adv_dis2_name)
    critic_name,critic_suffix=os.path.splitext(adv_critic_name)
    for epoch in range(param_dict["train_params"]["total_epoch"]):

        print("----------0----------")
        print1("Epoch: {}/{}  Adv".format(epoch, param_dict["train_params"]["total_epoch"]))
        model_dict, optimizer_dict, scheduler_dict = adversarial_train(epoch,eval_file=eval_file,
                                                                       eval_batchs_number=eval_batchs_number,eval_data_params=eval_data_params,
                                                                       real_eval_data_params=real_eval_data_params,model_dict=model_dict,
                                                                       optimizer_dict=optimizer_dict, scheduler_dict=scheduler_dict,
                                                                       dis_dataloader_params=dis_data_params, dis2_dataloader_params=dis2_data_params,
                                                                       vocab_size=vocab_size, pos_file=pos_file, neg_file=neg_file, batch_size=batch_size,
                                                                       use_cuda=use_cuda, epoch=epoch, tot_epoch=param_dict["train_params"]["total_epoch"])
        print("----------1----------")
        adv_gen_path=os.path.join(gen_name+str(epoch)+gen_suffix)
        adv_dis_path=os.path.join(dis_name+str(epoch)+dis_suffix)
        adv_dis2_path=os.path.join(disa_name+str(epoch)+disa_suffix)
        adv_critic_path=os.path.join(critic_name+str(epoch)+critic_suffix)


        torch.save(model_dict['generator'].state_dict(), adv_gen_path)
        torch.save(model_dict['discriminator'].state_dict(), adv_dis_path)
        torch.save(model_dict['discriminator_absolute'].state_dict(),adv_dis2_path)
        torch.save(model_dict['critic'].state_dict(), adv_critic_path)
        print("----------2----------")


        soga=15
        if epoch%soga==0 and not(epoch==0): #fix rl train with MLE
            change_lr(m, m_lr/100)
            change_lr(w, w_lr / 100)
            # MLE_epoch=param_dict["train_params"]["pre_gen_epoch_num"]+epoch//3
            MLE_epoch = 100000 + epoch //soga
            model_dict, optimizer_dict, scheduler_dict = pretrain_generator(MLE_epoch, is_load_pre_gen, pre_gen_path,
                                                                            model_dict, optimizer_dict, scheduler_dict,
                                                                            r_dataloader, vocab_size=vocab_size,
                                                                            eval_data_params=eval_data_params,
                                                                            eval_file=eval_file,
                                                                            eval_batchs_number=eval_batchs_number,
                                                                            use_cuda=use_cuda, epoch=epoch,
                                                                            tot_epochs=range(param_dict["train_params"][
                                                                                                 "pre_gen_epoch_num"]))
            change_lr(m, m_lr)
            change_lr(w, w_lr)

        print("----------3----------")



if __name__ == "__main__":
    main()
